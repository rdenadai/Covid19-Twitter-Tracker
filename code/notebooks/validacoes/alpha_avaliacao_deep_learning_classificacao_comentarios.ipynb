{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:38:11.901425Z",
     "start_time": "2020-06-12T13:38:11.898936Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(f\"{os.getcwd()}/../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:38:44.538444Z",
     "start_time": "2020-06-12T13:38:20.826657Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import warnings\n",
    "from itertools import chain\n",
    "from multiprocessing import cpu_count\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import ray\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import Constant\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, Activation, Flatten, \n",
    "    Embedding, Bidirectional, LSTM, GRU, Attention, \n",
    "    BatchNormalization, Conv1D, MaxPooling1D, TimeDistributed,\n",
    "    SpatialDropout1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    ")\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from src.data.processing.utils import CleanUp, RSLP_STEMMER, SNOWBALL_STEMMER, NLP_LEMMATIZER\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "clean_up = CleanUp(\n",
    "    remove_accentuation=False,\n",
    "    remove_4_comment=False,\n",
    "    remove_numbers=False,\n",
    ")\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:38:45.385494Z",
     "start_time": "2020-06-12T13:38:44.545453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:38:45.395542Z",
     "start_time": "2020-06-12T13:38:45.391283Z"
    }
   },
   "outputs": [],
   "source": [
    "max_words = 250_000\n",
    "batch_size = 128\n",
    "w2v_size = 300\n",
    "\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:32:20.164209Z",
     "start_time": "2020-06-10T02:32:19.992784Z"
    }
   },
   "outputs": [],
   "source": [
    "# ray.shutdown()\n",
    "# ray.init(num_cpus=cpu_count(), include_webui=False, lru_evict=True)\n",
    "\n",
    "# filename = f\"{os.getcwd()}/../../../data/embedding/corpus.txt\"\n",
    "# file_it = pd.read_csv(filename, header=None, iterator=True, names=[\"sentence\"], chunksize=1000,)\n",
    "\n",
    "# @ray.remote\n",
    "# def carregar_frases(lines):\n",
    "#     import os\n",
    "#     import sys\n",
    "\n",
    "#     sys.path.append(f\"{os.getcwd()}/../../\")\n",
    "#     from processing.utils import CleanUp, SNOWBALL_STEMMER\n",
    "#     clean_up = CleanUp(stemmer=SNOWBALL_STEMMER)\n",
    "    \n",
    "#     lines = lines[\"sentence\"].tolist()\n",
    "#     return [clean_up.fit(line) for line in lines]\n",
    "\n",
    "# i, itera = 0, []\n",
    "# for lines in file_it:\n",
    "#     itera.append(lines)\n",
    "#     if i == 10:\n",
    "#         break\n",
    "#     i += 1\n",
    "# corpus = list(chain(*ray.get([carregar_frases.remote(lines) for lines in itera])))\n",
    "\n",
    "# ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:38:47.372706Z",
     "start_time": "2020-06-12T13:38:45.397075Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{os.getcwd()}/../../data/processed/dataset.csv\", sep=\"|\")\n",
    "df[\"x\"] = df[\"comentario\"].apply(lambda comment: clean_up.fit(str(comment)))\n",
    "df[\"y\"] = df[\"classificacao\"].apply(lambda clasf: 0 if clasf == \"negativo\" else 1)\n",
    "textos = df[[\"x\", \"y\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:38:47.377390Z",
     "start_time": "2020-06-12T13:38:47.373838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cloroquina deve ser administrada aos pacientes logo início doença preferencialmente dia aparecimento dos primeiros sintomas como febre tosse coriza respiração superior vezes por minuto paolo zanoto virologista usp'\n",
      " 0]\n",
      "Dataset size: 2560\n",
      "Corpus size: 2560\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(textos))\n",
    "print(f'Dataset size: {len(textos)}')\n",
    "# print(df[df['x'] == ''].index)\n",
    "# print(df.loc[250])\n",
    "\n",
    "for item in list(textos[:, 0]):\n",
    "    corpus.append(item)\n",
    "print(f'Corpus size: {len(corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:32:22.506730Z",
     "start_time": "2020-06-10T02:32:22.189202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maxlen: 82\n"
     ]
    }
   ],
   "source": [
    "maxlen = 0\n",
    "for phrase in textos[:, 0]:\n",
    "    size = len(phrase.split())\n",
    "    maxlen = size if size > maxlen else maxlen\n",
    "maxlen += 1\n",
    "print(f'Maxlen: {maxlen}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:28:42.404202Z",
     "start_time": "2020-06-10T02:28:11.667Z"
    }
   },
   "outputs": [],
   "source": [
    "# word_counts = defaultdict(int)\n",
    "# for phrases in textos[:, 0]:\n",
    "#     for word in phrases.split():\n",
    "#         word_counts[word] += 1\n",
    "# v_count = len(word_counts.keys())\n",
    "# words_list = list(word_counts.keys())\n",
    "# word_index = dict((word, i) for i, word in enumerate(words_list))\n",
    "\n",
    "# X = []\n",
    "# for phrases in textos[:, 0]:\n",
    "#     xp = []\n",
    "#     for word in phrases.split():\n",
    "#         xp.append(word_index[word])\n",
    "#     X.append(xp)\n",
    "# y = textos[:, 1].astype(np.int).ravel()\n",
    "# y = to_categorical(y, classes)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=max_words)\n",
    "# X_train = tokenizer.sequences_to_matrix(X_train, mode='binary')\n",
    "# X_test = tokenizer.sequences_to_matrix(X_test, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:28:42.404807Z",
     "start_time": "2020-06-10T02:28:12.057Z"
    }
   },
   "outputs": [],
   "source": [
    "X = textos[:, 0]\n",
    "y = textos[:, 1].astype(np.int).ravel()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, lower=False, oov_token='<OOV>', char_level=False)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen, padding='post')\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    random_state=0,\n",
    "    n_jobs=-1,\n",
    "    max_iter=3000,\n",
    "    multi_class=\"ovr\"\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "df_cm = confusion_matrix(pred, y_test)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sn.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:38:56.810506Z",
     "start_time": "2020-06-12T13:38:53.880561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masculino => [('feminino', 0.8773196935653687), ('sexo', 0.8331319689750671), ('sexos', 0.7289565801620483)]\n",
      "sexo => [('masculino', 0.8331320285797119), ('feminino', 0.7899670600891113), ('sexos', 0.7375198602676392)]\n",
      "montanha => [('cume', 0.8779435753822327), ('everest', 0.8501795530319214), ('íngreme', 0.8373156785964966)]\n",
      "oceano => [('atlântico', 0.9057531356811523), ('antártida', 0.880797803401947), ('pacífico', 0.8795431852340698)]\n",
      "lua => [('vênus', 0.7782222032546997), ('saturno', 0.764536440372467), ('cheia', 0.7553818225860596)]\n",
      "amor => [('amar', 0.8543565273284912), ('felicidade', 0.8307334780693054), ('ama', 0.8102244734764099)]\n",
      "senhor => [('dai', 0.7575034499168396), ('digo', 0.7542639970779419), ('comigo', 0.7504541277885437)]\n",
      "medico => [('solicite', 0.9183236360549927), ('discuta', 0.8992379903793335), ('decidirá', 0.887068510055542)]\n",
      "cimegripe => [('tylenol', 0.9584749341011047), ('tylalgin', 0.948271632194519), ('resfenol', 0.9426720142364502)]\n",
      "passaro => [('porthos', 0.9917574524879456), ('revida', 0.991043746471405), ('alimentas', 0.991034209728241)]\n",
      "doenca => [('marketeira', 0.9931282997131348), ('bianconi', 0.9928562641143799), ('hypnosis', 0.9927748441696167)]\n",
      "coracao => [('terminei—', 0.9941285848617554), ('prezadíssimo', 0.9936602115631104), ('entristeças', 0.9934432506561279)]\n",
      "febre => [('calafrios', 0.7103829383850098), ('maculosa', 0.6997460126876831), ('39ºc', 0.6944174766540527)]\n",
      "nimesulida => [('uciton', 0.8718510866165161), ('betaciclodextrina', 0.8584884405136108), ('scaflam', 0.8480398058891296)]\n",
      "rancor => [('mágoa', 0.931958019733429), ('mágoas', 0.9307729005813599), ('maldade', 0.9265737533569336)]\n",
      "lobo => [('lobos', 0.722367525100708), ('coruja', 0.6980854272842407), ('pavão', 0.6790971755981445)]\n",
      "mau => [('funcionamento', 0.7010359168052673), ('ruim', 0.6751028299331665), ('prejudicado', 0.6617789268493652)]\n",
      "odio => [('magoas', 0.9787428379058838), ('merecê', 0.9784075021743774), ('acalentar', 0.9770864248275757)]\n",
      "dor => [('cefaleia', 0.7592452764511108), ('dores', 0.7435150742530823), ('cefaléia', 0.7421920895576477)]\n",
      "coriza => [('entupido', 0.9387205243110657), ('rinorreia', 0.9323529005050659), ('rinorréia', 0.9172753095626831)]\n",
      "braco => [('perdigueiro', 0.9871450662612915), ('basset', 0.98513263463974), ('dálmata', 0.9844822287559509)]\n",
      "['lobo', 'mau'] => [('agouro', 0.7087206244468689), ('nítido', 0.6943944692611694), ('perturbado', 0.6859896183013916)]\n",
      "maca => [('trancada', 0.9292829632759094), ('arremessado', 0.9263489842414856), ('necrotério', 0.9260653853416443)]\n",
      "coco => [('banana', 0.9029404520988464), ('guaraná', 0.8972501158714294), ('morango', 0.8864496946334839)]\n",
      "espada => [('armadura', 0.8927174806594849), ('flecha', 0.8724338412284851), ('dragão', 0.8569645881652832)]\n",
      "cavaleiro => [('dragão', 0.8493340611457825), ('espada', 0.8333823680877686), ('cetro', 0.8316925168037415)]\n",
      "arthur => [('simon', 0.8854528665542603), ('benjamin', 0.8850884437561035), ('samuel', 0.8837647438049316)]\n",
      "['rei', 'arthur'] => [('nero', 0.857555091381073), ('saul', 0.8548475503921509), ('luigi', 0.8525718450546265)]\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2Vec.load(f\"{os.getcwd()}/../../models/w2v.model\")\n",
    "\n",
    "for word in [\n",
    "    \"masculino\", \"sexo\", \"montanha\", \"oceano\", \"lua\", \"amor\", \"senhor\", \"medico\", \n",
    "    \"cimegripe\", \"passaro\", \"doenca\", \"coracao\", \"febre\", \"nimesulida\", \"rancor\",\n",
    "    \"lobo\", \"mau\", \"odio\", \"dor\", \"coriza\", \"braco\", [\"lobo\", \"mau\"], \"maca\", \"coco\",\n",
    "    \"espada\", \"cavaleiro\", \"arthur\", [\"rei\", \"arthur\"]\n",
    "]:\n",
    "    print(word, \"=>\", w2v.most_similar(word)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:28:42.406420Z",
     "start_time": "2020-06-10T02:28:12.717Z"
    }
   },
   "outputs": [],
   "source": [
    "X = textos[:, 0]\n",
    "y = textos[:, 1].astype(np.int).ravel()\n",
    "# y = to_categorical(y, 2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, lower=False, oov_token='<OOV>', char_level=False)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen, padding='post')\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:28:42.407256Z",
     "start_time": "2020-06-10T02:28:13.016Z"
    }
   },
   "outputs": [],
   "source": [
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:28:42.407969Z",
     "start_time": "2020-06-10T02:28:13.331Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weights = {i : class_weights[i] for i in range(2)}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:28:42.408694Z",
     "start_time": "2020-06-10T02:28:13.678Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, w2v_size))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = None\n",
    "    try:\n",
    "        embedding_vector = w2v.wv[word]\n",
    "    except:\n",
    "        pass\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Embedding Matrix size:', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:28:42.409551Z",
     "start_time": "2020-06-10T02:28:14.004Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train[0], X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:28:42.410648Z",
     "start_time": "2020-06-10T02:28:14.476Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential(name='WordEmbeddings')\n",
    "model.add(\n",
    "    Embedding(\n",
    "        len(word_index) + 1,\n",
    "        w2v_size, \n",
    "        weights=[embedding_matrix],\n",
    "        trainable=True,\n",
    "    )\n",
    ")\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Conv1D(128, 2, padding='valid', activation='relu', strides=1))\n",
    "# model.add(MaxPooling1D())\n",
    "# model.add(Conv1D(128, 2, padding='valid', activation='relu', strides=1))\n",
    "# model.add(MaxPooling1D())\n",
    "# model.add(Conv1D(128, 2, padding='valid', activation='relu', strides=1))\n",
    "# model.add(MaxPooling1D())\n",
    "# model.add(Conv1D(128, 2, padding='valid', activation='relu', strides=1))\n",
    "# model.add(MaxPooling1D())\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv1D(128, 2, padding='valid', activation='relu', strides=1))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling1D())\n",
    "# model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "# model.add(Flatten())\n",
    "# model.add(Conv1D(128, 2, padding='same', activation='relu', strides=1))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(\n",
    "#     Dense(\n",
    "#         128,\n",
    "#         activation='tanh',\n",
    "#         kernel_regularizer=regularizers.l2(1e-3),\n",
    "#         activity_regularizer=regularizers.l2(1e-3)\n",
    "#     )\n",
    "# )\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(\n",
    "    Dense(\n",
    "        128,\n",
    "        activation='elu',\n",
    "        kernel_regularizer=regularizers.l2(1e-4),\n",
    "        activity_regularizer=regularizers.l2(1e-4)\n",
    "    )\n",
    ")\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:28:42.411347Z",
     "start_time": "2020-06-10T02:28:14.883Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "checkpoint_filepath = './weights/weights.hdf5'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    verbose=0,\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs, \n",
    "    verbose=0,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[model_checkpoint_callback]\n",
    ")\n",
    "\n",
    "model.load_weights(checkpoint_filepath)\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', f'{round(score[1] * 100)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:28:42.412096Z",
     "start_time": "2020-06-10T02:28:15.164Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred = (model.predict(X_test) >= .5).astype(np.int).ravel()\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "# print(np.unique(y_test, return_counts=True))\n",
    "# print(np.unique(pred, return_counts=True))\n",
    "\n",
    "df_cm = confusion_matrix(y_test, pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sn.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:28:42.413020Z",
     "start_time": "2020-06-10T02:28:15.552Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Acc\", c=\"C0\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Val. Acc\", c=\"C2\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"Loss\", c=\"C1\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Val. Loss\", c=\"C3\")\n",
    "plt.xticks(range(0, epochs, 2))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T13:40:42.309062Z",
     "start_time": "2020-06-12T13:39:19.386382Z"
    }
   },
   "outputs": [],
   "source": [
    "d2v = Doc2Vec.load(f\"{os.getcwd()}/../../models/d2v.model\")\n",
    "\n",
    "n_textos = []\n",
    "for texto in textos:\n",
    "    doc_vec = d2v.infer_vector(texto[0].split(), epochs=500)\n",
    "    n_textos.append(doc_vec)\n",
    "n_textos = np.asarray(n_textos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:28:42.414283Z",
     "start_time": "2020-06-10T02:28:16.479Z"
    }
   },
   "outputs": [],
   "source": [
    "X = n_textos\n",
    "y = textos[:, 1].astype(np.int).ravel()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:28:42.414889Z",
     "start_time": "2020-06-10T02:28:16.703Z"
    }
   },
   "outputs": [],
   "source": [
    "# # For Conv & LSTM\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "X_train = X_train.reshape(2048, 300, -1)\n",
    "X_test = X_test.reshape(512, 300, -1)\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:28:42.415559Z",
     "start_time": "2020-06-10T02:28:16.909Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential(name='Doc2Vec')\n",
    "# model.add(Conv1D(128, 2, padding='valid', activation='elu'))\n",
    "# model.add(Conv1D(128, 2, padding='valid', activation='elu'))\n",
    "# model.add(Conv1D(128, 2, padding='valid', activation='elu'))\n",
    "# model.add(Conv1D(32, 5, padding='same', activation='elu'))\n",
    "# model.add(MaxPooling1D())\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:28:42.416175Z",
     "start_time": "2020-06-10T02:28:17.135Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "checkpoint_filepath_d2v = './weights/weights_d2v.hdf5'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath_d2v,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    verbose=0,\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs, \n",
    "    verbose=0,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[model_checkpoint_callback]\n",
    ")\n",
    "\n",
    "model.load_weights(checkpoint_filepath_d2v)\n",
    "score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', round(score[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:28:42.416753Z",
     "start_time": "2020-06-10T02:28:17.320Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = (model.predict(X_test) > 0.5).astype(np.int).ravel()\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "df_cm = confusion_matrix(y_test, pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sn.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T02:28:42.417526Z",
     "start_time": "2020-06-10T02:28:17.652Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Acc\", c=\"C0\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Val. Acc\", c=\"C2\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"Loss\", c=\"C1\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Val. Loss\", c=\"C3\")\n",
    "plt.xticks(range(0, epochs, 2))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
